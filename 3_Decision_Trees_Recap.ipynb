{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees Recap\n",
    "\n",
    "\n",
    "Now is the perfect time to recap everything you have learned about decision trees. \n",
    "\n",
    "By the way, this can also be done as a pair exercise! Explaining the concepts to another person allows you to challenge and deepen your own understanding. \n",
    "Here are two ways to do this:\n",
    "- a. we have created a sketch board where you can answer the questions together (you can both write on the same board).\n",
    "- b. you answer the questions together in this notebook.\n",
    "\n",
    "For a: In this repo, we have added an excalidraw file (\"Decision-Tree-Recap.excalidraw\"). You can open this at [excalidraw.com](https://excalidraw.com/) and start a \"live collaboration\" session with your Pair Programmer. Start with explaining the Decision Tree Algorithm to each other in 2 minutes.\n",
    "\n",
    "\n",
    "For b: Here are the questions:\n",
    "\n",
    "1. The typical interview question:\n",
    "<br />\n",
    "<br />\n",
    "Q: What is your favorite algorithm?\n",
    "<br />\n",
    "A: Decision trees\n",
    "<br />\n",
    "Q: Can you explain it to me in 2 minutes?\n",
    "<br />\n",
    "<br />\n",
    "2. Explain the tree terminology: node, threshold, leaf, and stump.\n",
    "<br />\n",
    "<br />\n",
    "3. Explain how to determine the optimal split \n",
    "<br />\n",
    "<br />\n",
    "4. Explain what the Gini impurity is and how to calculate it\n",
    "<br />\n",
    "<br />\n",
    "5. Explain how to make predictions for unseen data (especially with an impure leaf)\n",
    "<br />\n",
    "<br />\n",
    "6. How to avoid overfitting decision trees\n",
    "<br />\n",
    "<br />\n",
    "7. What are the advantages and disadvantages of decision trees?\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "\n",
    "8. Here is a sample data set. Please check your understanding and let's grow a tree on paper! \n",
    "\n",
    "    Data:\n",
    "    \n",
    "    |X1|X2 |Y|\n",
    "    |---|---|---|\n",
    "    |15|35|1|\n",
    "    |17|45|1|\n",
    "    |24|37|1|\n",
    "    |24|37|1|\n",
    "    |23|45|2|\n",
    "    |27|37|2|\n",
    "    |33|37|2|\n",
    "    |27|34|3|\n",
    "    |28|33|3|\n",
    "\n",
    "\n",
    "    Remember how to decide on the next division to be made.\n",
    "    - Please calculate the Gini gain for each step. \n",
    "    - You can also draw a scatterplot with all data points on paper and add the decision boundaries. \n",
    "    - Do you think there would have been another way to split the tree to get the same Gini gain at each step?\n",
    "    - To evaluate your grown tree, simply use the data and train a decision tree with Sklearn and plot your tree as you have seen in the notebook before.\n",
    "\n",
    "If you want to read up on how to make the next decision in your tree and how to calculate the Gini gain, here is a good resource: [Decision Tree from Scratch in Python](https://towardsdatascience.com/decision-tree-from-scratch-in-python-46e99dfea775). **Only the text!**\n",
    "\n",
    "---\n",
    "## Extras\n",
    "\n",
    "If you haven't had enough of decision trees yet, here are a few more ways to delve further into the topic:\n",
    "1. get deep into the code of building a dec tree from scratch in python\n",
    "2. get into the [theory of building trees recursively](https://cr.yp.to/2005-261/bender2/DT.pdf) (same as nr. 1 but without code)\n",
    "3. watch [another decision tree lecture](https://www.youtube.com/watch?v=wr9gUr-eWdA&list=PLEny7y9mZcaxFAyivwT2nwIEvfHAzFdX9&index=11), they are talking splitting criterions (entropy, gini etc) and regression trees. The part about decision trees ends after 1h.\n",
    "\n",
    "### To Nr 1\n",
    "You will see that in the [blogpost above](https://towardsdatascience.com/decision-tree-from-scratch-in-python-46e99dfea775) there is also an own Python implementation of the Decison Tree Algorithm at the end.\n",
    "This is an **extra**: If you want to dive deep into how to building a decision tree **recursively** in Python, check out the following code. \n",
    "\n",
    "This code is built like a python script: it starts with the definitions of the classes and their methods. At the very bottom the implemented Decision Tree Classifier is then applied to the iris dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T06:23:02.156460Z",
     "start_time": "2020-10-23T06:23:00.611175Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Implementation of the CART algorithm to train decision tree classifiers.\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, predicted_class):\n",
    "        self.predicted_class = predicted_class\n",
    "        self.feature_index = 0\n",
    "        self.threshold = 0\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build decision tree classifier.\"\"\"\n",
    "        self.n_classes_ = len(set(y))\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.tree_ = self._grow_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self._predict(inputs) for inputs in X]\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        \"\"\"Find the best split for a node.\n",
    "        \"Best\" means that the average impurity of the two children, weighted by their\n",
    "        population, is the smallest possible. Additionally it must be less than the\n",
    "        impurity of the current node.\n",
    "        To find the best split, we loop through all the features, and consider all the\n",
    "        midpoints between adjacent training samples as possible thresholds. We compute\n",
    "        the Gini impurity of the split generated by that particular feature/threshold\n",
    "        pair, and return the pair with smallest impurity.\n",
    "        Returns:\n",
    "            best_idx: Index of the feature for best split, or None if no split is found.\n",
    "            best_thr: Threshold to use for the split, or None if no split is found.\n",
    "        \"\"\"\n",
    "        # Need at least two elements to split a node.\n",
    "        m = y.size\n",
    "        if m <= 1:\n",
    "            return None, None\n",
    "\n",
    "        # Count of each class in the current node.\n",
    "        num_parent = [np.sum(y == c) for c in range(self.n_classes_)]\n",
    "\n",
    "        # Gini of current node.\n",
    "        best_gini = 1.0 - sum((n / m) ** 2 for n in num_parent)\n",
    "        best_idx, best_thr = None, None\n",
    "\n",
    "        # Loop through all features.\n",
    "        for idx in range(self.n_features_):\n",
    "            # Sort data along selected feature.\n",
    "            thresholds, classes = zip(*sorted(zip(X[:, idx], y)))\n",
    "\n",
    "            # We could actually split the node according to each feature/threshold pair\n",
    "            # and count the resulting population for each class in the children, but\n",
    "            # instead we compute them in an iterative fashion, making this for loop\n",
    "            # linear rather than quadratic.\n",
    "            num_left = [0] * self.n_classes_\n",
    "            num_right = num_parent.copy()\n",
    "            for i in range(1, m):\n",
    "                c = classes[i - 1]\n",
    "                num_left[c] += 1\n",
    "                num_right[c] -= 1\n",
    "                gini_left = 1.0 - sum(\n",
    "                    (num_left[x] / i) ** 2 for x in range(self.n_classes_)\n",
    "                )\n",
    "                gini_right = 1.0 - sum(\n",
    "                    (num_right[x] / (m - i)) ** 2 for x in range(self.n_classes_)\n",
    "                )\n",
    "\n",
    "                # The Gini impurity of a split is the weighted average of the Gini\n",
    "                # impurity of the children.\n",
    "                gini = (i * gini_left + (m - i) * gini_right) / m\n",
    "\n",
    "                # The following condition is to make sure we don't try to split two\n",
    "                # points with identical values for that feature, as it is impossible\n",
    "                # (both have to end up on the same side of a split).\n",
    "                if thresholds[i] == thresholds[i - 1]:\n",
    "                    continue\n",
    "\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_idx = idx\n",
    "                    best_thr = (thresholds[i] + thresholds[i - 1]) / 2\n",
    "\n",
    "        return best_idx, best_thr\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        \"\"\"Build a decision tree by recursively finding the best split.\"\"\"\n",
    "        # Population for each class in current node. The predicted class is the one with\n",
    "        # largest population.\n",
    "        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "        node = Node(predicted_class=predicted_class)\n",
    "\n",
    "        # Split recursively until maximum depth is reached.\n",
    "        if depth < self.max_depth:\n",
    "            idx, thr = self._best_split(X, y)\n",
    "            if idx is not None:\n",
    "                indices_left = X[:, idx] < thr\n",
    "                X_left, y_left = X[indices_left], y[indices_left]\n",
    "                X_right, y_right = X[~indices_left], y[~indices_left]\n",
    "                node.feature_index = idx\n",
    "                node.threshold = thr\n",
    "                node.left = self._grow_tree(X_left, y_left, depth + 1)\n",
    "                node.right = self._grow_tree(X_right, y_right, depth + 1)\n",
    "        return node\n",
    "\n",
    "    def _predict(self, inputs):\n",
    "        \"\"\"Predict class for a single sample.\"\"\"\n",
    "        node = self.tree_\n",
    "        while node.left:\n",
    "            if inputs[node.feature_index] < node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.predicted_class\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    from sklearn.datasets import load_iris\n",
    "\n",
    "    # Load data.\n",
    "    dataset = load_iris()\n",
    "    X, y = dataset.data, dataset.target  # pylint: disable=no-member\n",
    "\n",
    "    #train model\n",
    "    clf = DecisionTreeClassifier(max_depth=1)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    #predict on one sample\n",
    "    print(clf.predict([[0, 0, 5, 1.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's done when running clf.fit()\n",
    "\n",
    "If you're feeling a little lost in the code and recursive tree construction, hopefully this image can help you a bit with how the tree was built. Of course, it is only an example, including some possibilities why further splits didn't happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import SVG\n",
    "SVG(filename='Decision_Tree.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "a358f456b3db30f4960f1d1db189de3e0c085ae9275824c948008ba36bb41223"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
